# ğŸ§  Retrieval-Augmented Generation (RAG) â€“ Data Preparation with Langchain

This project is the first part of a RAG (Retrieval-Augmented Generation) pipeline using [Langchain](https://www.langchain.com/) â€” focused on preparing and embedding custom data for future LLM-based querying.

> âœ… **Implemented: Data Preparation**  
> ğŸ” **Next Up: Query & Generation**

---

## ğŸ“Œ What is RAG?

**RAG (Retrieval-Augmented Generation)** is a powerful architecture for building intelligent applications over your own data using LLMs. It combines traditional information retrieval with LLM-based generation, enabling more accurate and grounded responses.

---

## ğŸ§± Part 1 â€“ Data Preparation

This repo covers:

### 1. ğŸ” Ingestion

Supports multiple data sources:
- PDFs
- Text files
- JSON
- URLs (web scraping)
- CSV, Excel

### 2. âœ‚ï¸ Chunking

Text is split into manageable chunks using Langchainâ€™s document loaders and text splitters, considering token limits.

### 3. ğŸ§  Embedding

Each chunk is converted into a vector using your choice of:
- OpenAI embeddings
- HuggingFace models
- LLaMA or Gemini

### 4. ğŸ—ƒï¸ Vector Store

Embeddings are stored in a vector database for fast similarity-based retrieval. Currently supported:
- Pinecone
- ChromaDB (default, open-source)

---

## âš™ï¸ Tech Stack

- ğŸ¦œ Langchain
- ğŸ§  OpenAI Embeddings
- ğŸ” ChromaDB
- ğŸ Python 3.10+

---
